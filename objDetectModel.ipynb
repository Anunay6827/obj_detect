{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Code has been generalised so that you can upload your own datasets and paths to your pipeline\n"
      ],
      "metadata": {
        "id": "uQJv_zLiJlKp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "!pip install tf_slim\n",
        "!pip install opencv-python-headless\n",
        "!pip install lxml\n",
        "!pip install pillow\n",
        "!pip install numpy\n",
        "!pip install matplotlib\n",
        "!pip install Cython\n",
        "!pip install contextlib2\n",
        "!pip install jupyter\n",
        "!pip install pandas\n",
        "!pip install tf-models-official"
      ],
      "metadata": {
        "id": "xPU5xyc_KHrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from object_detection.utils import config_util\n",
        "from object_detection.builders import model_builder\n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "import cv2"
      ],
      "metadata": {
        "id": "N_GWuhzzJMVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the pipeline config file\n",
        "pipeline_config = 'path/to/your/pipeline.config'\n",
        "\n",
        "# Load pipeline config and build a detection model\n",
        "configs = config_util.get_configs_from_pipeline_file(pipeline_config)\n",
        "model_config = configs['model']\n",
        "detection_model = model_builder.build(model_config=model_config, is_training=True)\n"
      ],
      "metadata": {
        "id": "HtmgbSDXJYii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from object_detection.utils import dataset_util\n",
        "from object_detection.protos import input_reader_pb2\n",
        "from google.protobuf import text_format\n",
        "\n",
        "# Load the TFRecord dataset\n",
        "train_record_path = 'path/to/train.record'\n",
        "val_record_path = 'path/to/val.record'\n",
        "label_map_path = 'path/to/label_map.pbtxt'\n",
        "\n",
        "# Prepare the dataset\n",
        "def create_tf_example(example):\n",
        "    # Create a TFExample based on your data\n",
        "    pass\n",
        "\n",
        "def load_dataset(record_path):\n",
        "    raw_dataset = tf.data.TFRecordDataset(record_path)\n",
        "    parsed_dataset = raw_dataset.map(create_tf_example)\n",
        "    return parsed_dataset\n",
        "\n",
        "train_dataset = load_dataset(train_record_path)\n",
        "val_dataset = load_dataset(val_record_path)\n"
      ],
      "metadata": {
        "id": "sukFPv7gJZyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "steps_per_epoch = 100\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        "\n",
        "@tf.function\n",
        "def train_step(image_tensors, groundtruth_boxes_list, groundtruth_classes_list):\n",
        "    with tf.GradientTape() as tape:\n",
        "        preprocessed_images = tf.cast(image_tensors, dtype=tf.float32)\n",
        "        predictions = detection_model(preprocessed_images, training=True)\n",
        "        losses = detection_model.loss(predictions, groundtruth_boxes_list, groundtruth_classes_list)\n",
        "\n",
        "    gradients = tape.gradient(losses['Loss/total_loss'], detection_model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, detection_model.trainable_variables))\n",
        "    return losses\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for step, (images, boxes, classes) in enumerate(train_dataset.take(steps_per_epoch)):\n",
        "        losses = train_step(images, boxes, classes)\n",
        "        if step % 10 == 0:\n",
        "            print(f'Epoch {epoch + 1}, Step {step}, Loss: {losses[\"Loss/total_loss\"].numpy()}')\n"
      ],
      "metadata": {
        "id": "84efzrDKJb2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B4-LUWlIJebh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}